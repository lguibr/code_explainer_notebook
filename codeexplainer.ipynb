{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4afb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python libraries\n",
    "!python3 -m pip install --upgrade langchain deeplake openai gitpython tiktoken python-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import shutil\n",
    "from getpass import getpass\n",
    "from git import Repo\n",
    "\n",
    "# Set up API keys\n",
    "os.environ['OPENAI_API_KEY'] = getpass(\"OpenAi Token:\")\n",
    "os.environ['ACTIVELOOP_TOKEN'] = getpass('Activeloop Token:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clone repository\n",
    "def clone_repository(github_user, reponame, branch='main'):\n",
    "    repo_url = f'https://github.com/{github_user}/{reponame}.git'\n",
    "    local_path = os.path.join('.', reponame)\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        shutil.rmtree(local_path)\n",
    "    \n",
    "    repo = Repo.clone_from(repo_url, local_path, branch=branch)\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "github_user = 'lguibr'\n",
    "reponame = 'pongo'\n",
    "branch = 'main' # Optional, defaults to 'main'\n",
    "local_path = clone_repository(github_user, reponame, branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "import magic\n",
    "\n",
    "root_dir = local_path\n",
    "\n",
    "ignored_paths = [\n",
    "    '.git',\n",
    "    '/.venv/',\n",
    "    # Add more paths to ighere\n",
    "]\n",
    "\n",
    "\n",
    "def is_text_file(filepath):\n",
    "    file_type = magic.from_file(filepath)\n",
    "    return 'text' in file_type\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    print(f\"Directory: {dirpath}\")\n",
    "    if any(ignored_path in dirpath for ignored_path in ignored_paths):\n",
    "        print(f\"Ignoring the directory: {dirpath}\")\n",
    "        continue  # Skip this directory if it contains an ignored path\n",
    "\n",
    "    for file in filenames:\n",
    "        filepath = os.path.join(dirpath, file)\n",
    "        if is_text_file(filepath):\n",
    "            print(f\"Loading file: {file}\")\n",
    "            try:\n",
    "                loader = TextLoader(filepath, encoding='utf-8')\n",
    "                \n",
    "                content = loader.load_and_split()\n",
    "                print(f'filepath:\\n{filepath}\\n')\n",
    "                docs.extend(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file: {file}, error: {e}\")\n",
    "                pass\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "# After splitting documents\n",
    "print(f\"Created {len(texts)} texts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971eb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steup DEEPLAKE_ACCOUNT_NAME\n",
    "\n",
    "DEEPLAKE_ACCOUNT_NAME=\"lgpelin92\"\n",
    "DATASET_NAME=\"pongo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8fed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = DeepLake.from_documents(texts, embeddings, dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/{DATASET_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LangChain and DeepLake\n",
    "db = DeepLake(dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/{DATASET_NAME}\", read_only=True, embedding_function=embeddings)\n",
    "\n",
    "model='gpt-4'\n",
    "timeout=6000\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=model,\n",
    "    timeout=timeout,\n",
    "    streaming=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,   \n",
    ")\n",
    "\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bcd4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, FileLink, clear_output\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    question = input(\"Enter your question (type 'exit' to quit): \")\n",
    "    \n",
    "    if question.lower() == 'exit':\n",
    "        break\n",
    "    clear_output()\n",
    "    display(Markdown(f\"-> **Question**: {question}\"))\n",
    "    display(Markdown(f\"**Answer**:\"))\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    answer = result['answer']\n",
    "    clear_output()\n",
    "    display(Markdown(f\"-> **Question**: {question}\"))\n",
    "    display(Markdown(f\"**Answer**: {answer}\\n\"))\n",
    "    \n",
    "    chat_history.append((question, answer))\n",
    "\n",
    "clear_output()    \n",
    "display(Markdown(\"\\nChat history:\"))\n",
    "\n",
    "for q, a in chat_history:\n",
    "    display(Markdown(f\"-> **Question**: {q}\"))\n",
    "    display(Markdown(f\"**Answer**: {a}\\n\"))\n",
    "        \n",
    "## Save chat_history to a text file\n",
    "filename = \"chat_history.txt\"\n",
    "with open(filename, \"w\") as f:\n",
    "    for q, a in chat_history:\n",
    "        f.write(f\"-> **Question**: {q}\\n\")\n",
    "        f.write(f\"**Answer**: {a}\\n\\n\")\n",
    "\n",
    "# Offer user to download the file\n",
    "FileLink(filename, result_html_prefix=\"Click here to download chat history: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2bcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
